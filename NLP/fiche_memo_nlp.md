
# üß† Fiche M√©mo ‚Äì Pipeline NLP Classique (sans TF-IDF)

## üîÅ √âtapes du pipeline

### 1Ô∏è‚É£ Lecture du texte
```python
with open("fichier.txt", "r", encoding="utf-8") as f:
    texte = f.read()
```

### 2Ô∏è‚É£ Mise en minuscules
```python
texte = texte.lower()
```

### 3Ô∏è‚É£ Suppression de la ponctuation
```python
import re
texte = re.sub(r"[^\w\s]", " ", texte)
```

### 4Ô∏è‚É£ Tokenisation
```python
from nltk.tokenize import word_tokenize
nltk.download('punkt')  # une seule fois
tokens = word_tokenize(texte, language='french')
```

### 5Ô∏è‚É£ Suppression des stopwords
```python
from nltk.corpus import stopwords
nltk.download('stopwords')  # une seule fois
mots_utiles = [mot for mot in tokens if mot not in stopwords.words('french') and mot.isalpha()]
```

### 6Ô∏è‚É£ Lemmatisation (avec SpaCy)
```python
import spacy
nlp = spacy.load("fr_core_news_sm")
doc = nlp(" ".join(mots_utiles))
lemmes = [token.lemma_ for token in doc if token.lemma_ != ""]
```

## ‚úÖ R√©sultat final : liste propre de lemmes
Exemple :
```python
['rechercher', 'analyste', 'rh', 'analyser', 'indicateur', 'performance']
```

---

## üìå √Ä retenir
- `re` ‚Üí nettoyage avec regex
- `nltk.tokenize` ‚Üí d√©coupe en mots
- `stopwords` ‚Üí filtre les mots vides
- `spacy` ‚Üí lemmatisation pro en fran√ßais
